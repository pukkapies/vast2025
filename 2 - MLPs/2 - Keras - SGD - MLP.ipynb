{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning and Deep Learning\n",
    "### Keras / SGD / Multilayer perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following toy dataset to illustrate fitting a logistic regression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=150, centers=[[0.0, 5.0], [0.0, -5.0]], cluster_std=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='spring')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no closed form solution to this minimisation problem (unlike linear regression), so we use **gradient descent** to optimise the parameters.\n",
    "\n",
    "The `sklearn` library has a `LogisticRegression` class (similar to the `LinearRegression` class we used) that can be used to fit a logistic regression model, but from now we will choose to use `keras` for our model development, as it is more flexible and we can also use it for general deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Input(shape=(2,)),\n",
    "    Dense(1, activation='sigmoid')  # No activation: linear regression\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train the logistic regression model with **stochastic gradient descent** or SGD ("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(Xtrain, ytrain, epochs=50, batch_size=32)  # batch_size=32 is default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(Xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = model.variables[0].numpy().squeeze()\n",
    "b = model.variables[1].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x1 = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)\n",
    "x2 = (-b - w[0] * x1) / w[1]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(10, 3))\n",
    "ax1.set_title(\"Training set\")\n",
    "ax1.scatter(Xtrain[:, 0], Xtrain[:, 1], c=ytrain, s=50, cmap='spring')\n",
    "ax1.plot(x1, x2, c='b', linewidth=3)\n",
    "\n",
    "ax2.set_title(\"Test set\")\n",
    "ax2.scatter(Xtest[:, 0], Xtest[:, 1], c=ytest, s=50, cmap='spring')\n",
    "ax2.plot(x1, x2, c='b', linewidth=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer perceptrons\n",
    "\n",
    "The simplest type of deep learning model is the **multilayer perceptron**, also known as a **feedforward network**. This type of neural network can be viewed as an architecture consisting of layers of mathematical neurons, linked together in a directed acyclic graph.\n",
    "\n",
    "#### MLP with single hidden layer\n",
    "A key property of deep learning models is the fact that they are _compositional_ instead of _additive_. Where as linear regression models (or logistic regression) increase complexity by adding extra basis functions $\\phi_i$ in the expansion\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = \\sum_{i} w_i \\phi_i(\\mathbf{x}),\n",
    "$$\n",
    "\n",
    "deep learning models increase complexity by composing multiple simple functions $\\varphi_k$ together:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = \\varphi_L(\\varphi_{L-1}(\\ldots\\varphi_2(\\varphi_1(\\mathbf{x}))\\ldots )).\n",
    "$$\n",
    "\n",
    "The functions $\\varphi_k$ are defined to be affine transformations followed by an element-wise activation function. An example is the MLP with a single hidden layer: \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "h_j^{(1)} &= \\sigma\\left( \\sum_{i=1}^D w^{(0)}_{ji}x_i + b_j^{(0)} \\right),\\qquad j=1,\\ldots,n_h,\\\\\n",
    "\\hat{y} &= \\sigma_{out}\\left( \\sum_{i=1}^{n_h} w^{(1)}_{i}h^{(1)}_i + b^{(1)} \\right). \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In the above, $\\mathbf{x}\\in\\mathbb{R}^D$ is an example input, $n_h\\in\\mathbb{N}$ is the number of hidden units in the network, $\\sigma, \\sigma_{out}:\\mathbb{R}\\mapsto\\mathbb{R}$ are activation functions, $w^{(0)}_{ji}\\in\\mathbb{R}$ and $w^{(1)}_{ji}\\in\\mathbb{R}$ are weights, and $b_j^{(0)}\\in\\mathbb{R}$ and $b^{(1)}\\in\\mathbb{R}$ are biases.\n",
    "\n",
    "We will usually write equations (5) and (6) in the more concise form:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{h}^{(1)} &= \\sigma\\left( \\mathbf{W}^{(0)}\\mathbf{x} + \\mathbf{b}^{(0)} \\right),\\\\\n",
    "\\hat{y} &= \\sigma_{out}\\left( \\mathbf{w}^{(1)}\\mathbf{h}^{(1)} + b^{(1)} \\right),\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{x}\\in\\mathbb{R}^D$, $\\mathbf{W}^{(0)}\\in\\mathbb{R}^{n_h\\times D}$, $\\mathbf{b}^{(0)}\\in\\mathbb{R}^{n_h}$, $\\mathbf{h}^{(1)}\\in\\mathbb{R}^{n_h}$, $\\mathbf{w}^{(1)}\\in\\mathbb{R}^{1\\times n_h}$, $b^{(1)}\\in\\mathbb{R}$ and we overload notation with the activation functions $\\sigma, \\sigma_{out}: \\mathbb{R}\\mapsto\\mathbb{R}$ by applying them element-wise in the above.\n",
    "\n",
    "This hidden layer is a type of neural network layer that is often referred to as a **dense** or **fully connected** layer.\n",
    "\n",
    "#### MLP with multiple hidden layers\n",
    "More generally, for an MLP with $L$ hidden layers, we have\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{h}^{(0)} &:= \\mathbf{x}, \\\\\n",
    "\\mathbf{h}^{(k)} &= \\sigma\\left( \\mathbf{W}^{(k-1)}\\mathbf{h}^{(k-1)} + \\mathbf{b}^{(k-1)} \\right),\\qquad k=1,\\ldots, L,\\\\\n",
    "\\hat{y} &= \\sigma_{out}\\left( \\mathbf{w}^{(L)}\\mathbf{h}^{(L)} + b^{(L)} \\right), \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{W}^{(k)}\\in\\mathbb{R}^{n_{k+1}\\times n_k}$, $\\mathbf{b}^{(k)}\\in\\mathbb{R}^{n_{k+1}}$, $\\mathbf{h}^{(k)}\\in\\mathbb{R}^{n_k}$, and we have set $n_0 := D$, and $n_k$ is the number of units in the $k$-th hidden layer.\n",
    "\n",
    "The hidden layers inside a deep network can be viewed as *learned feature extractors*. The weights of the network learn to encode the data in such a way as to represent progressively more complex or abstract features of the data that are useful for solving the problem task at hand. This hierarchy of representations is a core property of the expressive power of deep learning models ([Rumelhart et al 1986b](#Rumelhart86))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP classifier example\n",
    "\n",
    "We will demonstrate the use of multiple hidden layers by fitting a classifier to the following 'two moons' dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=200, noise=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Input(shape=(2,)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(Xtrain, ytrain, epochs=2000, batch_size=16, validation_data=(Xtest, ytest), verbose=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='val')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx, ny = 50, 50\n",
    "\n",
    "x_ = np.linspace(X[:, 0].min(), X[:, 0].max(), nx)\n",
    "y_ = np.linspace(X[:, 1].min(), X[:, 1].max(), ny)\n",
    "\n",
    "X_, Y_ = np.meshgrid(x_, y_)\n",
    "\n",
    "inputs = np.transpose(np.stack([X_, Y_]), [1, 2, 0])\n",
    "inputs = np.reshape(inputs, (nx * ny, 2))\n",
    "\n",
    "Z = model(inputs).numpy()\n",
    "Z = np.reshape(Z, (nx, ny))\n",
    "\n",
    "plt.contour(X_, Y_, Z, levels=[0.5], cmap='RdGy')\n",
    "plt.scatter(Xtrain[:, 0], Xtrain[:, 1], c=ytrain, alpha=0.8)\n",
    "plt.scatter(Xtest[:, 0], Xtest[:, 1], c=ytest, marker='*', alpha=0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "* Robbins, H. and Monro, S. (1951), \"A stochastic approximation method\", *The annals of mathematical statistics*, 400â€“407.\n",
    "* Rumelhart, D. E., Hinton, G., & Williams, R. (1986), \"Learning representations by back-propagating errors\", Nature, **323**, 533-536."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
