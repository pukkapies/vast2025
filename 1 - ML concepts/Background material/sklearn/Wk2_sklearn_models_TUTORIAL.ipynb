{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su0twq-78bEF"
      },
      "source": [
        "# Week 2 - Scikit-learn (sklearn) models tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJUNezSI8bEG"
      },
      "source": [
        "In this tutorial, we will start working with the model fitting API in sklearn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKYJHNh38bEH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGrPp2jK8bEH"
      },
      "source": [
        "## k-nearest neighbours"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_6x4jmY8bEH"
      },
      "source": [
        "We first demonstrate the use of the k-NN algorithm on the Iris dataset.\n",
        "\n",
        "Recall that this dataset consists of measurements of three different species of irises."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvNKd1qR8bEI"
      },
      "source": [
        "#### Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8owYhr58bEI"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7G2Jevgo8bEI"
      },
      "outputs": [],
      "source": [
        "iris = datasets.load_iris()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7bfEOyH8bEI"
      },
      "outputs": [],
      "source": [
        "iris.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v12lHsNR8bEI"
      },
      "source": [
        "We first split the dataset into a training and validation set with a 80/20 split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qIgklsG8bEI"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBssOy7m8bEI"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(iris.data, iris.target, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FzMMddZ8bEJ"
      },
      "outputs": [],
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(Y_train.shape)\n",
        "print(Y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODytnuC78bEJ"
      },
      "source": [
        "### Build a kNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aQwZw5A8bEJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpaNgG3W8bEJ"
      },
      "outputs": [],
      "source": [
        "# Instantiate a model. n_neighbors is a hyperparameter of the model.\n",
        "knn = KNeighborsClassifier(n_neighbors=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_KEWiP88bEJ"
      },
      "outputs": [],
      "source": [
        "# The sklearn API uses a .fit() method\n",
        "knn.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lbMb_W_8bEJ"
      },
      "outputs": [],
      "source": [
        "Y_pred = knn.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVjF6FSL8bEJ"
      },
      "outputs": [],
      "source": [
        "metrics.accuracy_score(Y_test, Y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WkVNHA98bEJ"
      },
      "source": [
        "This model is quick to fit, so let's try a range of hyperparameter values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9Djl_lT8bEJ"
      },
      "outputs": [],
      "source": [
        "scores_list = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJrHH_8t8bEJ"
      },
      "outputs": [],
      "source": [
        "for k in range(1, 26):\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(X_train, Y_train)\n",
        "    Y_pred = knn.predict(X_test)\n",
        "    score = metrics.accuracy_score(Y_test, Y_pred)\n",
        "    scores_list.append(score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCt0Vc6K8bEK"
      },
      "source": [
        "Plot the accuracy as a function of $k$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqijYdUC8bEK"
      },
      "outputs": [],
      "source": [
        "plt.plot(range(1, 26), scores_list)\n",
        "plt.xlabel(\"Number of neighbours\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vg-9VLTR8bEK"
      },
      "source": [
        "As we see we have several hyperparameter choices that maximise accuracy. Select an optimal hyperparameter in the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyD6Bgv68bEK"
      },
      "outputs": [],
      "source": [
        "k = 10\n",
        "knn = KNeighborsClassifier(n_neighbors=k)\n",
        "knn.fit(iris.data, iris.target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_PcuPin8bEK"
      },
      "source": [
        "We can use our trained model to make predictions on new (unseen) data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRe0BQdx8bEK"
      },
      "outputs": [],
      "source": [
        "new_data = np.array([[3, 4, 5, 2]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRNO-Hlk8bEK"
      },
      "outputs": [],
      "source": [
        "classes = iris.target_names\n",
        "classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JlK6itS8bEK"
      },
      "outputs": [],
      "source": [
        "classes[knn.predict(new_data)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rirwmMUv8bEK"
      },
      "source": [
        "## Principal components analysis (PCA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkxafrTx8bEK"
      },
      "source": [
        "We now demonstrate the use of sklearn for PCA. We begin with a simple synthesised dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RLfxxBN8bEK"
      },
      "source": [
        "#### Create synthetic dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22MUAP1k8bEL"
      },
      "outputs": [],
      "source": [
        "P = np.array([[3, 4],\n",
        "             [1, 0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0EagRce8bEL"
      },
      "outputs": [],
      "source": [
        "X = np.random.randn(200, 2)\n",
        "X = np.dot(X, P)\n",
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhNF-Wzw8bEL"
      },
      "outputs": [],
      "source": [
        "plt.plot(X[:, 0], X[:, 1], 'o')\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_TGplVo8bEL"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "pca.fit(X)\n",
        "print(pca.explained_variance_)\n",
        "print(pca.components_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fUhTvTb8bEL"
      },
      "outputs": [],
      "source": [
        "plt.plot(X[:, 0], X[:, 1], 'o', alpha=0.5)\n",
        "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
        "    v = vector * 3 * np.sqrt(length)\n",
        "    plt.plot([0, v[0]], [0, v[1]], '-k', lw=3)\n",
        "plt.axis('equal');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nwjEs1b8bEL"
      },
      "source": [
        "#### Digits dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jq4ehpyn8bEL"
      },
      "outputs": [],
      "source": [
        "# Load in the `digits` data\n",
        "digits = datasets.load_digits()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdTwAAPc8bEL"
      },
      "outputs": [],
      "source": [
        "digits.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjtc1UeC8bEL"
      },
      "outputs": [],
      "source": [
        "pca = PCA(2)  # project from 64 to 2 dimensions\n",
        "Xproj = pca.fit_transform(digits.data)\n",
        "print(digits.data.shape)\n",
        "print(Xproj.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fl3Yiix8bEM"
      },
      "outputs": [],
      "source": [
        "plt.scatter(Xproj[:, 0], Xproj[:, 1], c=digits.target, edgecolor='none', alpha=0.5,\n",
        "            cmap=plt.cm.get_cmap('nipy_spectral', 10))\n",
        "plt.colorbar();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6dxTw_G8bEM"
      },
      "source": [
        "### Explained variance as a function of components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m45o-K388bEM"
      },
      "source": [
        "The more components we include, the more variance is explained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4PuYNlj8bEM"
      },
      "outputs": [],
      "source": [
        "pca = PCA().fit(digits.data)\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.xlabel('number of components')\n",
        "plt.ylabel('cumulative explained variance');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsR0pJLK8bEM"
      },
      "source": [
        "We can see that we lose a lot of information with just 2 components. We would need around 20 components to retain 90% of the variance.\n",
        "\n",
        "We can also see the data points as coded by an increasing number of principal components:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XA9pRO5x8bEM"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(8, 8, figsize=(8, 8))\n",
        "fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
        "\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    pca = PCA(i + 1).fit(digits.data)\n",
        "    im = pca.inverse_transform(pca.transform(digits.data[20:21]))\n",
        "\n",
        "    ax.imshow(im.reshape((8, 8)), cmap='binary')\n",
        "    ax.text(0.95, 0.05, 'n = {0}'.format(i + 1), ha='right',\n",
        "            transform=ax.transAxes, color='green')\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5_TY9X38bEM"
      },
      "source": [
        "## $k$-means clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gf3iE-_Z8bEM"
      },
      "source": [
        "We will look at an example of the $k$-means clustering algorithm, using the Titanic dataset. You can find the dataset [here](https://www.kaggle.com/c/titanic)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5aEhT938bEM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5Yl7FuO8bEM"
      },
      "outputs": [],
      "source": [
        "train_url = \"http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/train.csv\"\n",
        "train_set = pd.read_csv(train_url)\n",
        "test_url = \"http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/test.csv\"\n",
        "test_set = pd.read_csv(test_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqYRyvu68bEN"
      },
      "outputs": [],
      "source": [
        "train_set.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6f95gdL8bEN"
      },
      "outputs": [],
      "source": [
        "train_set.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajdJa4pj8bEN"
      },
      "source": [
        "### Data cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5heHEU38bEN"
      },
      "source": [
        "First inspect how many missing values there are in each columns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pSlIoHr8bEN"
      },
      "outputs": [],
      "source": [
        "train_set.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPTnuezX8bEN"
      },
      "source": [
        "#### Impute missing values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n24dX-c58bEN"
      },
      "source": [
        "One simple way of imputing missing values is to use the column mean. The `Cabin` and `Embarked` columns are not numerica so we cannot calculate a mean for these, so we will first drop the non-numeric columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcQiPEeE8bEO"
      },
      "outputs": [],
      "source": [
        "train_set.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUbkIMVq8bEO"
      },
      "source": [
        "We see that the `Name`, `Sex`, `Ticket`, `Cabin`, `Embarked` columns are non-numeric. We will drop all of these apart from `Sex` from the dataframes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNYebL-w8bEO"
      },
      "outputs": [],
      "source": [
        "train_set = train_set.drop(['Name','Ticket', 'Cabin','Embarked'], axis=1)\n",
        "test_set = test_set.drop(['Name','Ticket', 'Cabin','Embarked'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_set['Age'].mean()"
      ],
      "metadata": {
        "id": "0NHxgrfj9g0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Qiie_YL8bEN"
      },
      "outputs": [],
      "source": [
        "train_set.fillna(value={\"Age\": train_set['Age'].mean()}, inplace=True)\n",
        "test_set.fillna(value={\"Age\": test_set['Age'].mean()}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eBXc-dO8bEN"
      },
      "outputs": [],
      "source": [
        "train_set.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLf63rhM8bEN"
      },
      "outputs": [],
      "source": [
        "test_set.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "493527iv8bEO"
      },
      "source": [
        "#### Encode binary column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqVO2lnL8bEO"
      },
      "source": [
        "We will convert the `Sex` column to binary numeric values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_bhAQDy8bEO"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "labelEncoder = LabelEncoder()\n",
        "labelEncoder.fit(train_set['Sex'])\n",
        "labelEncoder.fit(test_set['Sex'])\n",
        "train_set['Sex'] = labelEncoder.transform(train_set['Sex'])\n",
        "test_set['Sex'] = labelEncoder.transform(test_set['Sex'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbnBdueV8bEO"
      },
      "outputs": [],
      "source": [
        "train_set.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3_x40H08bEO"
      },
      "outputs": [],
      "source": [
        "test_set.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-m_o6Yjc8bEO"
      },
      "source": [
        "### Train the $k$-means model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejYaEj-K8bEO"
      },
      "outputs": [],
      "source": [
        "X = np.array(train_set.drop(['Survived'], axis=1).astype(float))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aREEfuYL8bEP"
      },
      "source": [
        "The `Survived` column is the 'label'. We will not use it for $k$-means clustering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oj-piK628bEP"
      },
      "outputs": [],
      "source": [
        "y = np.array(train_set['Survived'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEUFyCVw8bEP"
      },
      "source": [
        "For $k$-means to work more effectively, we need to make sure that all features are on similar scales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkpPflCC8bEP"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyTOW9UQ8bEP"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "kmeans = KMeans(n_clusters=2, max_iter=600)\n",
        "kmeans.fit(X_scaled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vg3tInK8bEP"
      },
      "source": [
        "#### Calculate how well the clusters automatically separate the passengers who survived from those who didn't"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V08gHKgu8bEP"
      },
      "outputs": [],
      "source": [
        "correct = 0\n",
        "for i in range(len(X_scaled)):\n",
        "    row = np.array(X_scaled[i].astype(float))\n",
        "    prediction = kmeans.predict(np.expand_dims(row, axis=0))\n",
        "    if prediction[0] == y[i]:\n",
        "        correct += 1\n",
        "\n",
        "perc_correct = np.max([correct/len(X), 1-(correct/len(X))])\n",
        "print(f\"Automatically clustered {100*perc_correct:.2f}% of the data into survived/non survived clusters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeK6KhaH8bEP"
      },
      "source": [
        "## Sklearn estimators API\n",
        "\n",
        "Sklearn aims to have a consistent API for models (or *estimators*) in its library. Below are methods that appear for different classes of estimators.\n",
        "\n",
        "* **All models / estimators**\n",
        "  - `model.fit()`: fits the model to the training data. This method applies to both supervised algorithms (where it accepts two arguments: `model.fit(X, y)`). Unsupervised algorithms accept a single argument: `model.fit(X)`.\n",
        "* **Supervised algorithms**\n",
        "  - `model.predict()`: after fitting, a trained model makes a prediction for unseen data `X_test` with this method (`model.predict(X_test)`).\n",
        "  - `model.predict_proba()`: Some classifiers have this method available to return the probability of each label for unseen test data.\n",
        "* **Unsupervised algorithms**\n",
        "  - `model.predict()`: predict labels in clustering algorithms.\n",
        "  - `model.transform()`: transforms new data into the new basis: `model.transform(X_test)`."
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "feedforward",
      "language": "python",
      "name": "feedforward"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}