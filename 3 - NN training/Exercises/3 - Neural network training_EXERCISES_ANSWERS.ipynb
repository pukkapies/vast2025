{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network training exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Load the [diabetes dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html) from `sklearn`. Extract the inputs and targets from the dataset into numpy arrays, and convert the numpy arrays to have float32 type.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "diabetes_dataset = load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(diabetes_dataset[\"DESCR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(diabetes_dataset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = diabetes_dataset[\"data\"].astype(np.float32)\n",
    "targets = diabetes_dataset[\"target\"].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape, targets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Create training and validation splits with a 80/20 ratio. Compute the mean $\\mu_{train}$ and standard deviation $\\sigma_{train}$ of the targets from the training split, and normalise the training and validation targets $y$ by computing $(y - \\mu_{train})/\\sigma_{train}$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, val_data, train_targets, val_targets = train_test_split(data, targets, test_size=0.2) \n",
    "\n",
    "print(train_data.shape)\n",
    "print(val_data.shape)\n",
    "print(train_targets.shape)\n",
    "print(val_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_train = train_targets.mean()\n",
    "std_train = train_targets.std()\n",
    "print(mu_train, std_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets = (train_targets - mu_train) / std_train\n",
    "val_targets = (val_targets - mu_train) / std_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Define an MLP model to train on the diabetes dataset. Your model should have three hidden layers of size 256 neurons each, and using a ReLU activation function. The final layer should have a single neuron with no activation function, to predict the target value.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(train_data.shape[-1],)),\n",
    "    Dense(256, activation=\"relu\"),\n",
    "    Dense(256, activation=\"relu\"),\n",
    "    Dense(256, activation=\"relu\"),\n",
    "    Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Compile your model with the MSE loss and Adam optimizer. Train the model for 100 epochs, passing the validation data to the `validation_data` argument. Plot the training and validation curves. Are there signs of overfitting/underfitting?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=\"mse\")\n",
    "history = model.fit(train_data, train_targets, epochs=100, validation_data=(val_data, val_targets), verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Loss vs. epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Re-define your model by adding dropout after each hidden layer with a dropout rate of 0.5, and adding L2 regularisation to each hidden layer with a regularisation coefficient of 1e-5. Re-compile and re-train your new model, again for 100 epochs. Plot the training and validation curves. Has the regularisation made a difference?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "from keras import regularizers\n",
    "\n",
    "l2_coeff = 1e-5\n",
    "rate = 0.5\n",
    "\n",
    "def get_regularised_model():\n",
    "    model = Sequential([\n",
    "        Input(shape=(train_data.shape[-1],)),\n",
    "        Dense(256, kernel_regularizer=regularizers.l2(l2_coeff), activation=\"relu\"),\n",
    "        Dropout(rate),\n",
    "        Dense(256, kernel_regularizer=regularizers.l2(l2_coeff), activation=\"relu\"),\n",
    "        Dropout(rate),\n",
    "        Dense(256, kernel_regularizer=regularizers.l2(l2_coeff), activation=\"relu\"),\n",
    "        Dropout(rate),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    return model\n",
    "model = get_regularised_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=\"mse\")\n",
    "history = model.fit(train_data, train_targets, epochs=100, validation_data=(val_data, val_targets), verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Loss vs. epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Re-initialise the same regularised model from Q5. This time, compile the model including a mean absolute error (MAE) metric, and train for 100 epochs using early stopping, where the early stopping is monitoring validation MAE performance, and has a patience of 10 epochs. Plot the training and validation curves.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_regularised_model()\n",
    "model.compile(optimizer='adam', loss=\"mse\", metrics=['mae']) \n",
    "earlystopping = keras.callbacks.EarlyStopping(monitor='val_mae', patience=10)\n",
    "history = model.fit(train_data, train_targets, epochs=100, validation_data=(val_data, val_targets), verbose=False,\n",
    "                    callbacks=[earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot(history.history['val_mae'])\n",
    "ymax, ymin = plt.gca().get_ylim()\n",
    "plt.vlines(earlystopping.best_epoch, ymax=ymax, ymin=ymin, linestyle='--', color='r')\n",
    "plt.title('Loss vs. epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.xticks(np.arange(len(history.history['loss'])))\n",
    "plt.legend(['Training', 'Val loss', 'Val MAE', 'Best epoch'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
