{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network training exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Load the [diabetes dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html) from `sklearn`. Extract the inputs and targets from the dataset into numpy arrays, and convert the numpy arrays to have float32 type.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Create training and validation splits with a 80/20 ratio. Compute the mean $\\mu_{train}$ and standard deviation $\\sigma_{train}$ of the targets from the training split, and normalise the training and validation targets $y$ by computing $(y - \\mu_{train})/\\sigma_{train}$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Define an MLP model to train on the diabetes dataset. Your model should have three hidden layers of size 256 neurons each, and using a ReLU activation function. The final layer should have a single neuron with no activation function, to predict the target value.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Compile your model with the MSE loss and Adam optimizer. Train the model for 100 epochs, passing the validation data to the `validation_data` argument. Plot the training and validation curves. Are there signs of overfitting/underfitting?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Re-define your model by adding dropout after each hidden layer with a dropout rate of 0.5, and adding L2 regularisation to each hidden layer with a regularisation coefficient of 1e-5. Re-compile and re-train your new model, again for 100 epochs. Plot the training and validation curves. Has the regularisation made a difference?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Re-initialise the same regularised model from Q5. This time, compile the model including a mean absolute error (MAE) metric, and train for 100 epochs using early stopping, where the early stopping is monitoring validation MAE performance, and has a patience of 10 epochs. Plot the training and validation curves.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
