{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning and Deep Learning\n",
    "### Neural network training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimisers\n",
    "\n",
    "There are two main steps to training neural networks:\n",
    "\n",
    "1. Computation of the (stochastic) gradient of the loss function with respect to the model parameters\n",
    "2. Use of the computed gradient to update the parameters\n",
    "\n",
    "In step 1., the gradients of the loss with respect to the parameters can be efficiently computed using the backpropagation algorithm. In step 2., there are several popular gradient-based optimisation algorithms used in deep learning. Many are available as built-in optimisers in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weight regularisation and early stopping\n",
    "\n",
    "Deep learning models are typically very over-parameterised, often with millions of parameters over many layers in the model. They are universal approximators (see e.g. [Cybenko](#Cybenko89) for the large width case, or [Lu et al](#Lu17) for the large depth case), and so overfitting can be a problem. When training neural networks, it is important to regularise them to prevent overfitting. \n",
    "\n",
    "**$\\mathcal{l}^2$ and $\\mathcal{l}^1$ regularisation.** Recall that for a linear model of the form\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = \\sum_j w_j \\phi_j(\\mathbf{x}),\n",
    "$$\n",
    "\n",
    "a typical regularisation is to add a sum of squares penalty term to the loss term to discourage the weights $w_j$ from growing too large. In this case, the regularised loss takes the form\n",
    "\n",
    "\n",
    "$$\n",
    "L(\\mathbf{w}, \\alpha) = L_0(\\mathbf{w}) + \\alpha_2 \\sum_i w_i^2,\n",
    "$$\n",
    "\n",
    "where $L_0$ is the unconstrained loss function, and $\\alpha_2$ is a regularisation hyperparameter. This is $\\mathcal{l}^2$ regularisation.\n",
    "\n",
    "This form of regularisation is often referred to as **weight decay**, although the two terms are technically not the same. Weight decay ([Hanson & Pratt](#Hanson88)) is defined as a modification to the update rule, rather than to the loss function itself:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\theta}_{t+1} \\leftarrow (1 - \\lambda)\\theta_t - \\eta g_t,\n",
    "$$\n",
    "\n",
    "where $\\theta\\in\\mathbb{R}^p$ is the model parameters, $\\lambda$, $\\eta$ are hyperparameters, and $g_t$ is the $t$-th batch update. In the case of stochastic gradient descent, the update $g_t = \\nabla_\\theta L(\\theta_t; \\mathcal{D}_m)$ and the two formulations are equivalent. However, this is not the case for all gradient-based optimisers commonly used in deep learning.\n",
    "\n",
    "An alternative weight regularisation is $\\mathcal{l}^1$ regularisation, in which the sum of absolute values of the weights are added to the loss term:\n",
    "\n",
    "$$\n",
    "L(\\mathbf{w}, \\alpha) = L_0(\\mathbf{w}) + \\alpha_1 \\sum_i |w_i|.\n",
    "$$\n",
    "\n",
    "This form of regularisation encourages sparsity in the weights. Both $\\mathcal{l}^1$ and $\\mathcal{l}^2$ regularisation discourage the weights from growing too large, which restricts the capacity of the network.\n",
    "\n",
    "It is also possible to add a weighted combination of both $\\mathcal{l}^2$ and $\\mathcal{l}^1$ regularisation to the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Early stopping**. You might have found in the last week that it is difficult to set a good number of epochs to train for ahead of time. In toy examples the training is usually quick so it is not a problem to experiment, but in many cases training could take hours or days (or even longer!) and so this is not an option. \n",
    "\n",
    "Recall that deep learning models are usually vastly overparameterised, and have the capacity to drastically overfit. A simple but effective method is to simply stop the training before the model starts to overfit. \n",
    "\n",
    "With early stopping, the aim is to stop the training when the validation error is at a minimum. This means that the model needs to be regularly evaluated on a held-out validation set (that is not used for training), and the optimisation routine is terminated when the validation error starts to rise. Validation is normally performed once per epoch in the training run.\n",
    "\n",
    "In practice, the validation error measurements will be noisy, and so it is not a reliable measure to simply detect when the validation error increases and immediately stop the training. What is usually done is to periodically save model checkpoints (say once per epoch), and set a **patience** threshold, to specify a maximum number of validation runs that are allowed where the validation error does not improve upon the best score so far. If this patience threshold is reached, the training is terminated.\n",
    "\n",
    "The early stopping algorithm is outlined in pseudocode below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping inputs: `val_metric`, `max_patience`\n",
    "\n",
    "-------\n",
    ">```\n",
    ">best_valid_loss = np.inf\n",
    ">patience = 0\n",
    ">\n",
    ">for epoch in range(max_epochs):\n",
    ">    epoch_train_loss = train_model(train_data, train_loss)\n",
    ">    epoch_valid_loss = validate_model(valid_data, val_metric)\n",
    ">    if epoch_valid_loss < best_valid_loss:\n",
    ">        best_valid_loss = epoch_valid_loss\n",
    ">        save_model(epoch)\n",
    ">        patience = 0\n",
    ">    else:\n",
    ">        patience += 1\n",
    ">    \n",
    ">    if patience >= max_patience:\n",
    ">        break  # terminate training\n",
    ">```\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to validate the model using a measure that is different to the loss function used for training the model. Therefore `val_metric` is also an input to the early stopping algorithm above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras regularisers, Dropout layers and callbacks\n",
    "\n",
    "In this notebook we will build on what we have covered already with the `Sequential` API, and include weight regularisers, `Dropout` layers, and introduce callback objects - these are very useful objects for dynamically performing operations during the training run. An example is the `EarlyStopping` callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def target_f(x):\n",
    "    return np.sin(2 * np.pi * x)\n",
    "\n",
    "np.random.seed(16)\n",
    "X = np.random.rand(100, 1) * 2 - 1\n",
    "y = target_f(X) + np.random.normal(0, 0.1, size=X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=[x_train.shape[1]]),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(y_train.shape[1])\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "history = model.fit(x_train, y_train, epochs=100, validation_data=(x_test, y_test), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 3))\n",
    "\n",
    "ax1.plot(history.history['loss'], label='train')\n",
    "ax1.plot(history.history['val_loss'], label='val')\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"MSE Loss\")\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(history.history['mae'], label='train')\n",
    "ax2.plot(history.history['val_mae'], label='val')\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"MAE\")\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout & weight regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "from keras.regularizers import l2\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=[x_train.shape[1]]),\n",
    "    Dense(100, activation='relu', kernel_regularizer=l2(1e-3)),\n",
    "    Dropout(0.1),\n",
    "    Dense(100, activation='relu', kernel_regularizer=l2(1e-3)),\n",
    "    Dropout(0.1),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(y_train.shape[1])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "history = model.fit(x_train, y_train, epochs=100, validation_data=(x_test, y_test), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 3))\n",
    "\n",
    "ax1.plot(history.history['loss'], label='train')\n",
    "ax1.plot(history.history['val_loss'], label='val')\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"MSE Loss\")\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(history.history['mae'], label='train')\n",
    "ax2.plot(history.history['val_mae'], label='val')\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"MAE\")\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "earlystopping = EarlyStopping(patience=20, monitor='val_loss', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Input(shape=[x_train.shape[1]]),\n",
    "    Dense(100, activation='relu', kernel_regularizer=l2(1e-3)),\n",
    "    Dropout(0.1),\n",
    "    Dense(100, activation='relu', kernel_regularizer=l2(1e-3)),\n",
    "    Dropout(0.1),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(y_train.shape[1])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "history = model.fit(x_train, y_train, epochs=200, validation_data=(x_test, y_test), verbose=0, \n",
    "                    callbacks=[earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 3))\n",
    "\n",
    "ax1.plot(history.history['loss'], label='train')\n",
    "ax1.plot(history.history['val_loss'], label='val')\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"MSE Loss\")\n",
    "ymax, ymin = ax1.get_ylim()\n",
    "ax1.vlines(earlystopping.best_epoch, ymax=ymax, ymin=ymin, linestyle='--', color='r', label='Best epoch')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(history.history['mae'], label='train')\n",
    "ax2.plot(history.history['val_mae'], label='val')\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"MAE\")\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "* Chen, J. & Kyrillidis, A., (2019), \"Decaying Momentum Helps Neural Network Training\", arXiv preprint arXiv:1910.04952.\n",
    "* Cybenko, G. (1989) \"Approximations by superpositions of sigmoidal functions\", Mathematics of Control, Signals, and Systems, **2** (4), 303–314.\n",
    "* Hanson, S. J. & Pratt, L. Y. (1988) \"Comparing biases for minimal network construction with back-propagation\", in *Proceedings of the 1st International Conference on Neural Information Processing Systems*,  177–185.\n",
    "* Kingma, D. P. & Ba, J. L. (2015), \"Adam: a Method for Stochastic Optimization\", International Conference on Learning Representations, 1–13.\n",
    "* Lu, Z., Pu, H., Wang, F. Hu, Z., & Wang, L. (2017) \"The Expressive Power of Neural Networks: A View from the Width\", Advances in Neural Information Processing Systems 30. Curran Associates, Inc., 6231–6239.\n",
    "* Nesterov, Y. (1983), \"A method for unconstrained convex minimization problem with the rate of convergence o(1/k2)\", Doklady ANSSSR (translated as Soviet. Math. Docl.), **269**, 543–547.\n",
    "* Qian, N. (1999), \"On the momentum term in gradient descent learning algorithms\", Neural Networks: The Official Journal of the International Neural Network Society, **12** (1), 145–151.\n",
    "* Robbins, H. and Monro, S. (1951), \"A stochastic approximation method\", *The annals of mathematical statistics*, 400–407."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
